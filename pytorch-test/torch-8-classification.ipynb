{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "# create circle\n",
    "X, y = make_circles(n_samples=n_samples, noise=0.03, random_state=42)\n",
    "\n",
    "print(f'First 5 samples of X: \\n {X[:5]}')\n",
    "print(f'First 5 samples of y: \\n {y[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = pd.DataFrame({ \"X1\": X[:, 0], \"X2\": X[:, 1], \"label\": y })\n",
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Check input and output shapes\n",
    "\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "\n",
    "print(f'Values for one sample of X: {X_sample} and the y: {y_sample}')\n",
    "print(f\"Shapes for one sample of X: {X_sample.shape} and y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.from_numpy(X).type(torch.float64)\n",
    "y_torch = torch.from_numpy(y).type(torch.float64)\n",
    "\n",
    "type(X_torch), X_torch.dtype, y_torch.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_torch, y_torch, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)\n",
    "\n",
    "type(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Building a model\n",
    "1. setup device agnostic code for gpu/cpu\n",
    "2. construct model by subclassing (nn.module)\n",
    "3. Define loss function and optimizers\n",
    "3. create train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# make device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_feature= 2  (2, )\n",
    "X_train.shape\n",
    "\n",
    "# out_features= 1\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "# 1. subclass nn.module\n",
    "# 2. 'nn.Linear()' layers that are capable of handling the shapes of our data\n",
    "# 3. define forward() methods that outlines the forward computation of the model\n",
    "# 4. instantiate instance of our model class and send it to the target device\n",
    "\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2.\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # take 2 features and upscales to 5 features\n",
    "        # takes in 5 features and outputs 1 single features same shape as y\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)\n",
    "    # 3.\n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x.double())) # x -> layer_1 -> layer_2\n",
    "\n",
    "\n",
    "# 4. Instantiate the model class and send it to the target device\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Device: {device}')\n",
    "next(model_0.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets replicate the model above using nn.Sequential()\n",
    "\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "    \n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model predictions\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f'Length of predictions: {len(untrained_preds)}, shape: {untrained_preds.shape}')\n",
    "print(f'Length of test samples: {len(X_test)}, shape: {X_test.shape}')\n",
    "print(f'\\nFirst 10 predictions: \\n {torch.round(untrained_preds[:10])}')\n",
    "print(f'\\nFirst 10 labels: \\n {y_test[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:10], y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 setup loss function and optimizer\n",
    "\n",
    "which optimizer or loass we should use? it problem specific\n",
    "1. regression -> MAE or MSE\n",
    "2. classification -> cross entropy / binary cross entropy\n",
    "\n",
    "it measures the how wrong the predictions are\n",
    "\n",
    "Optimizers -> SGD / ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 setup loss\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # sigmoid activation built in\n",
    "\n",
    "# optimize the model parameter as the model weights in such a way that loss is reduced \n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy - out of 100 examples, what percentage does our model get\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model\n",
    "\n",
    "build training loop\n",
    "\n",
    "1. forward pass \n",
    "2. calculate the loss\n",
    "3. optimizer zero grad\n",
    "4. Loss backward (backpropagation)\n",
    "5. Optimizer step (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 raw logits -> prediction probabilities -> prediction labels\n",
    "# logits -> raw outputs of a models is called logits\n",
    "\n",
    "# i. convert logits into prediction probabilities by passing them to some \n",
    "# kind of activation function\n",
    "\n",
    "# ii. convert the prediction probabilities to prediction labels by either \n",
    "# rounding them or taking argmax()\n",
    "\n",
    "# nb: view the first 5 outputs of the forward pass on the test data\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_0(X_test.to(device))[:5]\n",
    "\n",
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sigmoid activation to our model logits\n",
    "# convert the predictions into predicted labels\n",
    "y_preds_probs = torch.sigmoid(y_logits)\n",
    "\n",
    "y_preds_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our predictions probability values, we need to perform a range-style rounding on them:\n",
    "\n",
    "`predictions >= 0.5 , y = 1 (class 1)`\n",
    "`prediction < 0.5, y = 0 (class 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predicted labels\n",
    "\n",
    "torch.round(y_preds_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the predicted labels\n",
    "y_preds = torch.round(y_preds_probs)\n",
    "\n",
    "# in full (logits -> pred probs -> pred labels)\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device=device))[:5]))\n",
    "\n",
    "# check for equality\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# get rid of extra dimension\n",
    "y_preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 building training and test loop\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# set epochs\n",
    "epochs = 100\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# building the training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. forward pass\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_preds = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, # bceWithLogits expects raw logits\n",
    "                   y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_preds)\n",
    "\n",
    "    # 3. optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. loss backward (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. forward pass\n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_preds = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. calculate test loss / acc\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_preds)\n",
    "    \n",
    "    # print what happens\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f} | Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. make predictions and evaluate the model\n",
    "\n",
    "we import a function called `plot_decision_boundary()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# download helper function\n",
    "\n",
    "if Path('helper_funcitons.py').is_file():\n",
    "    print('helper_funtions.py exists')\n",
    "else:\n",
    "    print('Downloading helper_functions.py')\n",
    "    request = requests.get('https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')\n",
    "    with open('helper_functions.py', 'wb') as f:\n",
    "        f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary of the model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model_0, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Improving the model (model perspective)\n",
    "\n",
    "1. Add more layers - give the model more chances to learn about pattersn in the data\n",
    "2. add more hidden units - go for 5 to 10\n",
    "3. fit for longer\n",
    "4. change the activate functions\n",
    "5. change the learning rate\n",
    "6. change the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # z = self.layer_1(x)\n",
    "        # z = self.layer_2(z)\n",
    "        # z = self.layer_3(z)\n",
    "\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "    \n",
    "\n",
    "model_1 = CircleModelV1().to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training and evaluation loop\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# put data on targe device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    model_1.train()\n",
    "    # 1. forward pass\n",
    "    y_logits = model_1(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probability -> pred labels\n",
    "\n",
    "    # 2. calculate loss\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss forward (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### testing \n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. forward pass\n",
    "        test_logits = model_1(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. create loss\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # status\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f} | Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model_1, X_train, y_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model_1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 preparing data to see if model can fit a stright line\n",
    "# creating data\n",
    "\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.01\n",
    "\n",
    "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y_regression = weight * X_regression + bias\n",
    "\n",
    "# create train and test split\n",
    "train_split = int(0.8 * len(X_regression))\n",
    "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
    "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
    "\n",
    "len(X_train_regression), len(X_test_regression), len(y_train_regression), len(y_test_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_data=X_train_regression, \n",
    "                 train_labels=y_train_regression,\n",
    "                 test_data=X_test_regression,\n",
    "                 test_labels=y_test_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 adjust model_1 to fit a stright line\n",
    "# same architecture as model_1 \n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(in_features=1, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
    "                            lr=0.01)\n",
    "\n",
    "# train the model\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# epochs\n",
    "epochs = 1000\n",
    "\n",
    "# put the data on target device\n",
    "X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n",
    "X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model_2(X_train_regression)\n",
    "    loss = loss_fn(y_pred, y_train_regression)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # testing\n",
    "    model_2.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_2(X_test_regression)\n",
    "        test_loss = loss_fn(test_pred, y_test_regression)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f} | Test Loss: {test_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on evaluation model\n",
    "\n",
    "model_2.eval()\n",
    "\n",
    "# make predictions\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_2(X_test_regression)\n",
    "\n",
    "# plot data and predictions\n",
    "plot_predictions(train_data=X_train_regression,\n",
    "                 train_labels=y_train_regression,\n",
    "                 test_data=X_test_regression,\n",
    "                 test_labels=y_test_regression,\n",
    "                 predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THe missing piece: non-linearity\n",
    "what patterns could you draw if you were given an infinite amount of a stright and non-stright lines?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 recreating non-linear data(red, blue) circles\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "X, y = make_circles(n_samples=n_samples, noise=0.03, random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensors\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.float16)\n",
    "y = torch.from_numpy(y).type(torch.float16)\n",
    "\n",
    "# train test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float16, device=device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float16, device=device)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float16, device=device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float16, device=device)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 - building a model with non linearity\n",
    "\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU() # non-linear activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # where we should put our non-linear function\n",
    "        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "    \n",
    "model_3 = CircleModelV2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss and optimizer\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1)\n",
    "\n",
    "# spam or not spam\n",
    "# credit cards = fraud or not \n",
    "# insurance claims = at fault or not fault\n",
    "\n",
    "# 6.3 train model with non linearity\n",
    "# random seed\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# put all data on target device\n",
    "X_train, y_train = X_train.float().to(device), y_train.float().to(device)\n",
    "X_test, y_test = X_test.float().to(device), y_test.float().to(device)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    model_3.train()\n",
    "\n",
    "    # 1. forward pass\n",
    "    y_logits = model_3(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "    # 2. calculate the loss\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. step\n",
    "    optimizer.step()\n",
    "\n",
    "    # testing\n",
    "    model_3.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model_3(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probability -> prediction labels\n",
    "        \n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # print\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss {loss:.5f} | Accuracy {acc:.2f} | Test loss {test_loss:.5f} | Test Accuracy {test_acc:.2f}% |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset models parameters\n",
    "def reset_models_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "# model_3.state_dict()\n",
    "# reset_models_weights(model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a model trained with non-linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_3(X_test).squeeze()\n",
    "    y_preds = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "print(y_preds[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model_1, X_train, y_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model_3, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 - replicating non-linear activation function\n",
    "\n",
    "Neural networks, rather than telling us the model what to learn, we give it the tools to discover patterns in data and it tries to figure out the patterns on its own.\n",
    "\n",
    "And these tools are linear & non-linear functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor\n",
    "A = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
    "\n",
    "# plt.plot(A)\n",
    "# plt.plot(torch.relu(A))\n",
    "def relu(x):\n",
    "    return torch.maximum(torch.tensor(0), x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# plot ReLu activation function\n",
    "plt.plot(relu(A))\n",
    "plt.plot(sigmoid(A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamflow-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
