{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "sys.path.insert(0, './yolov5')\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "# from yolov5.utils.datasets import LoadImages, LoadStreams\n",
    "from yolov5.utils.general import check_img_size, non_max_suppression, check_imshow, xyxy2xywh\n",
    "# scale_coords\n",
    "from yolov5.utils.torch_utils import select_device, time_sync\n",
    "from yolov5.utils.plots import Annotator, colors\n",
    "import torch\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIR_PATH = os.getcwd()\n",
    "TEST_IMAGE_PATH = os.path.join(DIR_PATH, 'test-images')\n",
    "YOLO_MODELS_PATH = os.path.join(DIR_PATH, 'yolo-models')\n",
    "\n",
    "# Load yolo models\n",
    "yolov5x = os.path.join(YOLO_MODELS_PATH, 'yolov5x.pt')\n",
    "yolov5n = os.path.join(YOLO_MODELS_PATH, 'yolov5n.pt')\n",
    "yolo5x6_b20231223 = os.path.join(YOLO_MODELS_PATH, 'yolo5x6_b20231223.pt')\n",
    "yolov5x6_b20240119_ct = os.path.join(YOLO_MODELS_PATH, 'yolov5x6_b20240119_ct.pt')\n",
    "yolov5x6_l20240119_ct = os.path.join(YOLO_MODELS_PATH, 'yolov5x6_l20240119_ct.pt')\n",
    "\n",
    "# Load attributes model\n",
    "faceProto = os.path.join('attributes-models', 'deploy.prototxt')\n",
    "faceModel =  os.path.join('attributes-models', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "ageProto = os.path.join('attributes-models', 'age_deploy.prototxt')\n",
    "ageModel = os.path.join('attributes-models', 'age_net.caffemodel')\n",
    "genderProto = os.path.join('attributes-models', 'gender_deploy.prototxt')\n",
    "genderModel = os.path.join('attributes-models', 'gender_net.caffemodel')\n",
    "\n",
    "face_net = cv2.dnn.readNetFromCaffe(faceProto, faceModel)\n",
    "age_net = cv2.dnn.readNetFromCaffe(ageProto, ageModel)\n",
    "gender_net = cv2.dnn.readNetFromCaffe(genderProto, genderModel)\n",
    "\n",
    "age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "gender_list = ['Male', 'Female']\n",
    "undefined = 'undefined'\n",
    "\n",
    "# Load test images\n",
    "test_img_1 = os.path.join(TEST_IMAGE_PATH, 'test1.png')\n",
    "test_img_2 = os.path.join(TEST_IMAGE_PATH, 'test2.png')\n",
    "test_img_3 = os.path.join(TEST_IMAGE_PATH, 'test3.png')\n",
    "test_img = os.path.join(TEST_IMAGE_PATH, 'test.jpg')\n",
    "\n",
    "IMGSZ = 1280\n",
    "RTSP_URL = 'rtsp://admin:L2140092@192.168.53.48:554/cam/realmonitor?channel=1&subtype=00'\n",
    "source = RTSP_URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(RTSP_URL):\n",
    "    cap = cv2.VideoCapture(RTSP_URL)\n",
    "\n",
    "    if cap.isOpened() is False:\n",
    "        print('[exit] can not access the camera.')\n",
    "        exit(0)\n",
    "\n",
    "    while True:\n",
    "        grabbed, frame = cap.read()\n",
    "\n",
    "        if cv2.waitKey(1) and 0xFF == ord('d'):\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2024-1-24 torch 2.1.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pred_xyxy: [tensor([[1.0910e+03, 6.6225e+02, 1.3283e+03, 7.9484e+02, 9.0248e-01, 2.0000e+00],\n",
      "        [1.3181e+03, 4.6120e+02, 1.4252e+03, 6.5272e+02, 8.5946e-01, 5.0000e+00],\n",
      "        [1.2299e+03, 4.3958e+02, 1.2837e+03, 5.1544e+02, 7.1532e-01, 2.0000e+00],\n",
      "        [9.0549e+01, 4.9778e+02, 1.1551e+02, 5.6647e+02, 7.0163e-01, 0.0000e+00],\n",
      "        [1.1528e+03, 4.9892e+02, 1.1735e+03, 5.5678e+02, 6.7624e-01, 0.0000e+00],\n",
      "        [1.7535e+03, 6.2000e+02, 1.7942e+03, 6.9566e+02, 6.6970e-01, 0.0000e+00],\n",
      "        [1.1244e+03, 3.9332e+02, 1.1419e+03, 4.3548e+02, 6.6714e-01, 0.0000e+00],\n",
      "        [1.8164e+01, 4.9090e+02, 4.6212e+01, 5.5276e+02, 6.5474e-01, 0.0000e+00],\n",
      "        [1.1125e+03, 4.5928e+02, 1.1371e+03, 5.2594e+02, 6.5209e-01, 0.0000e+00],\n",
      "        [1.7392e+02, 4.9923e+02, 1.9482e+02, 5.5787e+02, 4.6038e-01, 0.0000e+00],\n",
      "        [1.4006e-01, 4.5221e+02, 2.0696e+01, 5.4270e+02, 4.1396e-01, 0.0000e+00],\n",
      "        [1.1865e+03, 3.6011e+02, 1.2020e+03, 4.0069e+02, 3.8174e-01, 0.0000e+00],\n",
      "        [9.9949e+02, 5.0106e+02, 1.0212e+03, 5.5870e+02, 3.5564e-01, 0.0000e+00],\n",
      "        [1.0271e+03, 5.3302e+02, 1.0489e+03, 5.8533e+02, 3.4290e-01, 0.0000e+00],\n",
      "        [1.7871e+03, 0.0000e+00, 1.8198e+03, 2.5236e+01, 3.2037e-01, 7.4000e+01],\n",
      "        [1.1634e+03, 4.3977e+02, 1.1845e+03, 4.9169e+02, 3.1550e-01, 0.0000e+00],\n",
      "        [1.4541e+02, 4.5184e+02, 1.6897e+02, 5.3852e+02, 3.0029e-01, 0.0000e+00],\n",
      "        [1.1547e+03, 4.3473e+02, 1.1768e+03, 4.9027e+02, 2.9615e-01, 0.0000e+00]])]\n",
      "\n",
      "pred_xywh: [tensor([[1.2097e+03, 7.2854e+02, 2.3732e+02, 1.3259e+02, 9.0248e-01, 2.0000e+00],\n",
      "        [1.3716e+03, 5.5696e+02, 1.0705e+02, 1.9153e+02, 8.5946e-01, 5.0000e+00],\n",
      "        [1.2568e+03, 4.7751e+02, 5.3855e+01, 7.5864e+01, 7.1532e-01, 2.0000e+00],\n",
      "        [1.0303e+02, 5.3213e+02, 2.4965e+01, 6.8688e+01, 7.0163e-01, 0.0000e+00],\n",
      "        [1.1631e+03, 5.2785e+02, 2.0780e+01, 5.7856e+01, 6.7624e-01, 0.0000e+00],\n",
      "        [1.7739e+03, 6.5783e+02, 4.0660e+01, 7.5656e+01, 6.6970e-01, 0.0000e+00],\n",
      "        [1.1332e+03, 4.1440e+02, 1.7547e+01, 4.2162e+01, 6.6714e-01, 0.0000e+00],\n",
      "        [3.2188e+01, 5.2183e+02, 2.8047e+01, 6.1861e+01, 6.5474e-01, 0.0000e+00],\n",
      "        [1.1248e+03, 4.9261e+02, 2.4552e+01, 6.6661e+01, 6.5209e-01, 0.0000e+00],\n",
      "        [1.8437e+02, 5.2855e+02, 2.0898e+01, 5.8640e+01, 4.6038e-01, 0.0000e+00],\n",
      "        [1.0418e+01, 4.9745e+02, 2.0556e+01, 9.0498e+01, 4.1396e-01, 0.0000e+00],\n",
      "        [1.1942e+03, 3.8040e+02, 1.5549e+01, 4.0581e+01, 3.8174e-01, 0.0000e+00],\n",
      "        [1.0104e+03, 5.2988e+02, 2.1762e+01, 5.7633e+01, 3.5564e-01, 0.0000e+00],\n",
      "        [1.0380e+03, 5.5917e+02, 2.1784e+01, 5.2308e+01, 3.4290e-01, 0.0000e+00],\n",
      "        [1.8034e+03, 1.2618e+01, 3.2716e+01, 2.5236e+01, 3.2037e-01, 7.4000e+01],\n",
      "        [1.1739e+03, 4.6573e+02, 2.1078e+01, 5.1921e+01, 3.1550e-01, 0.0000e+00],\n",
      "        [1.5719e+02, 4.9518e+02, 2.3566e+01, 8.6677e+01, 3.0029e-01, 0.0000e+00],\n",
      "        [1.1657e+03, 4.6250e+02, 2.2108e+01, 5.5542e+01, 2.9615e-01, 0.0000e+00]])]\n",
      "\n",
      "\n",
      "\n",
      "pred_xyxy_norm: [tensor([[5.8500e-01, 6.7370e-01, 7.1224e-01, 8.0858e-01, 9.0248e-01, 2.0000e+00],\n",
      "        [7.0677e-01, 4.6917e-01, 7.6417e-01, 6.6401e-01, 8.5946e-01, 5.0000e+00],\n",
      "        [6.5945e-01, 4.4718e-01, 6.8833e-01, 5.2435e-01, 7.1532e-01, 2.0000e+00],\n",
      "        [4.8552e-02, 5.0639e-01, 6.1938e-02, 5.7627e-01, 7.0163e-01, 0.0000e+00],\n",
      "        [6.1810e-01, 5.0755e-01, 6.2924e-01, 5.6641e-01, 6.7624e-01, 0.0000e+00],\n",
      "        [9.4023e-01, 6.3072e-01, 9.6204e-01, 7.0769e-01, 6.6970e-01, 0.0000e+00],\n",
      "        [6.0289e-01, 4.0012e-01, 6.1230e-01, 4.4301e-01, 6.6714e-01, 0.0000e+00],\n",
      "        [9.7395e-03, 4.9938e-01, 2.4778e-02, 5.6231e-01, 6.5474e-01, 0.0000e+00],\n",
      "        [5.9653e-01, 4.6722e-01, 6.0969e-01, 5.3503e-01, 6.5209e-01, 0.0000e+00],\n",
      "        [9.3256e-02, 5.0786e-01, 1.0446e-01, 5.6751e-01, 4.6038e-01, 0.0000e+00],\n",
      "        [7.5097e-05, 4.6003e-01, 1.1097e-02, 5.5209e-01, 4.1396e-01, 0.0000e+00],\n",
      "        [6.3618e-01, 3.6634e-01, 6.4452e-01, 4.0762e-01, 3.8174e-01, 0.0000e+00],\n",
      "        [5.3592e-01, 5.0973e-01, 5.4759e-01, 5.6836e-01, 3.5564e-01, 0.0000e+00],\n",
      "        [5.5071e-01, 5.4224e-01, 5.6239e-01, 5.9545e-01, 3.4290e-01, 0.0000e+00],\n",
      "        [9.5822e-01, 0.0000e+00, 9.7576e-01, 2.5672e-02, 3.2037e-01, 7.4000e+01],\n",
      "        [6.2381e-01, 4.4738e-01, 6.3511e-01, 5.0020e-01, 3.1550e-01, 0.0000e+00],\n",
      "        [7.7965e-02, 4.5966e-01, 9.0601e-02, 5.4783e-01, 3.0029e-01, 0.0000e+00],\n",
      "        [6.1913e-01, 4.4224e-01, 6.3098e-01, 4.9875e-01, 2.9615e-01, 0.0000e+00]])]\n",
      "\n",
      "pred_xywh_norm: [tensor([[6.4862e-01, 7.4114e-01, 1.2725e-01, 1.3488e-01, 9.0248e-01, 2.0000e+00],\n",
      "        [7.3547e-01, 5.6659e-01, 5.7399e-02, 1.9484e-01, 8.5946e-01, 5.0000e+00],\n",
      "        [6.7389e-01, 4.8577e-01, 2.8877e-02, 7.7176e-02, 7.1532e-01, 2.0000e+00],\n",
      "        [5.5245e-02, 5.4133e-01, 1.3386e-02, 6.9876e-02, 7.0163e-01, 0.0000e+00],\n",
      "        [6.2367e-01, 5.3698e-01, 1.1142e-02, 5.8857e-02, 6.7624e-01, 0.0000e+00],\n",
      "        [9.5114e-01, 6.6920e-01, 2.1802e-02, 7.6964e-02, 6.6970e-01, 0.0000e+00],\n",
      "        [6.0760e-01, 4.2157e-01, 9.4088e-03, 4.2891e-02, 6.6714e-01, 0.0000e+00],\n",
      "        [1.7259e-02, 5.3085e-01, 1.5039e-02, 6.2930e-02, 6.5474e-01, 0.0000e+00],\n",
      "        [6.0311e-01, 5.0113e-01, 1.3164e-02, 6.7813e-02, 6.5209e-01, 0.0000e+00],\n",
      "        [9.8858e-02, 5.3769e-01, 1.1205e-02, 5.9654e-02, 4.6038e-01, 0.0000e+00],\n",
      "        [5.5860e-03, 5.0606e-01, 1.1022e-02, 9.2063e-02, 4.1396e-01, 0.0000e+00],\n",
      "        [6.4035e-01, 3.8698e-01, 8.3372e-03, 4.1283e-02, 3.8174e-01, 0.0000e+00],\n",
      "        [5.4175e-01, 5.3904e-01, 1.1669e-02, 5.8630e-02, 3.5564e-01, 0.0000e+00],\n",
      "        [5.5655e-01, 5.6885e-01, 1.1680e-02, 5.3213e-02, 3.4290e-01, 0.0000e+00],\n",
      "        [9.6699e-01, 1.2836e-02, 1.7542e-02, 2.5672e-02, 3.2037e-01, 7.4000e+01],\n",
      "        [6.2946e-01, 4.7379e-01, 1.1302e-02, 5.2819e-02, 3.1550e-01, 0.0000e+00],\n",
      "        [8.4283e-02, 5.0375e-01, 1.2636e-02, 8.8176e-02, 3.0029e-01, 0.0000e+00],\n",
      "        [6.2505e-01, 4.7049e-01, 1.1854e-02, 5.6503e-02, 2.9615e-01, 0.0000e+00]])]\n",
      "\n",
      "[     xcenter   ycenter     width    height  confidence  class    name\n",
      "0   0.648620  0.741140  0.127249  0.134883    0.902479      2     car\n",
      "1   0.735468  0.566593  0.057399  0.194837    0.859460      5     bus\n",
      "2   0.673887  0.485766  0.028877  0.077176    0.715319      2     car\n",
      "3   0.055245  0.541329  0.013386  0.069876    0.701627      0  person\n",
      "4   0.623668  0.536980  0.011142  0.058857    0.676244      0  person\n",
      "5   0.951136  0.669204  0.021802  0.076964    0.669701      0  person\n",
      "6   0.607596  0.421569  0.009409  0.042891    0.667138      0  person\n",
      "7   0.017259  0.530850  0.015039  0.062930    0.654740      0  person\n",
      "8   0.603108  0.501126  0.013164  0.067813    0.652093      0  person\n",
      "9   0.098858  0.537686  0.011205  0.059654    0.460377      0  person\n",
      "10  0.005586  0.506058  0.011022  0.092063    0.413960      0  person\n",
      "11  0.640349  0.386978  0.008337  0.041283    0.381738      0  person\n",
      "12  0.541751  0.539045  0.011669  0.058630    0.355637      0  person\n",
      "13  0.556549  0.568845  0.011680  0.053213    0.342904      0  person\n",
      "14  0.966993  0.012836  0.017542  0.025672    0.320368     74   clock\n",
      "15  0.629457  0.473787  0.011302  0.052819    0.315502      0  person\n",
      "16  0.084283  0.503746  0.012636  0.088176    0.300285      0  person\n",
      "17  0.625053  0.470495  0.011854  0.056503    0.296148      0  person]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "# yolo_model = DetectMultiBackend(yolov5x, device='cpu', dnn=True)\n",
    "# yolo_model = torch.hub.load(model=yolov5x)\n",
    "yolo_model = torch.hub._load_local('./yolov5/', 'custom', path=yolov5x)\n",
    "frame = cv2.imread(test_img_1)\n",
    "pred = yolo_model(frame)\n",
    "\n",
    "# print(pred.pandas().xyxy[0])\n",
    "# _pred = non_max_suppression(pred, 0.03, 0.5, 0, False, 500)\n",
    "\n",
    "pred_xyxy = pred.xyxy\n",
    "pred_xywh = pred.xywh\n",
    "\n",
    "pred_xyxy_norm = pred.xyxyn\n",
    "pred_xywh_norm = pred.xywhn\n",
    "\n",
    "print(f\"\"\"\n",
    "pred_xyxy: {pred_xyxy}\\n\n",
    "pred_xywh: {pred_xywh}\\n\\n\n",
    "\n",
    "pred_xyxy_norm: {pred_xyxy_norm}\\n\n",
    "pred_xywh_norm: {pred_xywh_norm}\n",
    "\"\"\")\n",
    "\n",
    "print(pred.pandas().xywhn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "half &= device.type != 'cpu'\n",
    "imgsz = [1280]\n",
    "\n",
    "yolo_model = DetectMultiBackend(yolov5x, device='cpu', dnn=True)\n",
    "\n",
    "stride, names, pt, jit, _ = yolo_model.stride, yolo_model.names, yolo_model.pt, yolo_model.jit, yolo_model.onnx\n",
    "\n",
    "# Half\n",
    "half &= pt and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA\n",
    "if pt:\n",
    "    yolo_model.model.half() if half else yolo_model.model.float()\n",
    "\n",
    "dataset = LoadImages(test_img_1, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
    "\n",
    "names = yolo_model.module.names if hasattr(yolo_model, 'module') else yolo_model.names\n",
    "\n",
    "if pt and device.type != 'cpu':\n",
    "    yolo_model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(yolo_model.model.parameters())))  # warmup\n",
    "dt, seen = [0.0, 0.0, 0.0, 0.0], 0\n",
    "\n",
    "for frame_idx, (path, img, im0s, vid_cap, s) in enumerate(dataset):\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if False else img.float()\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    pred = yolo_model(img, augment=False, visualize=False)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# get_face(frame)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m frameOpenCvDnn \u001b[38;5;241m=\u001b[39m pred_xywh_norm \u001b[38;5;66;03m# (720, 1280, 3)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m frameHeight \u001b[38;5;241m=\u001b[39m \u001b[43mframeOpenCvDnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m frameWidth \u001b[38;5;241m=\u001b[39m frameOpenCvDnn\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     40\u001b[0m frame_blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(frameOpenCvDnn, scalefactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     41\u001b[0m                                  size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m), mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m104\u001b[39m, \u001b[38;5;241m117\u001b[39m, \u001b[38;5;241m123\u001b[39m],\n\u001b[1;32m     42\u001b[0m                                  swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m                                  ddepth\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mCV_32F) \u001b[38;5;66;03m# nd.array\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "CONF_THRES = 0.05\n",
    "\n",
    "def predict_attribute(face, frame):\n",
    "    gender = undefined\n",
    "    age = undefined\n",
    "    padding = 20\n",
    "    \n",
    "    # print(face[0])\n",
    "    # print(face[1])\n",
    "    # print(face[2])\n",
    "    # print(face[3])\n",
    "    face_img = frame[max(0, face[1] - padding): min(face[3] + padding, frame.shape[0] - 1),\n",
    "                     max(0, face[0] - padding): min(face[2] + padding, frame.shape[1] - 1)]\n",
    "    \n",
    "    # print(frame.shape)\n",
    "\n",
    "    # print(frame[max(0, face[1] - padding): min(face[3] + padding, frame.shape[0] - 1)])\n",
    "    # print()\n",
    "    # print(max(0, face[0] - padding))\n",
    "    # print(min(face[2] + padding, frame.shape[1] - 1))\n",
    "    # if len(face_img) != 0:\n",
    "    #     blob = cv2.dnn.blobFromImage(face_img, scalefactor=1.0, size=(227, 227), mean=MODEL_MEAN_VALUES, swapRB=False)\n",
    "    #     gender_net.setInput(blob)\n",
    "    #     gender_preds = gender_net.forward()\n",
    "    #     gender = gender_list[gender_preds[0].argmax()]\n",
    "\n",
    "    #     age_net.setInput(blob)\n",
    "    #     age_preds = age_net.forward()\n",
    "    #     age = age_list[age_preds[0].argmax()]\n",
    "        \n",
    "    #     return gender, age\n",
    "    # print(face_img)\n",
    "    return 'hello', 'world'\n",
    "\n",
    "# get_face(frame)\n",
    "frameOpenCvDnn = pred_xywh_norm # (720, 1280, 3)\n",
    "# frameHeight = frameOpenCvDnn.shape[0]\n",
    "# frameWidth = frameOpenCvDnn.shape[1]\n",
    "\n",
    "\n",
    "frame_blob = cv2.dnn.blobFromImage(frameOpenCvDnn, scalefactor=1.0,\n",
    "                                 size=(300, 300), mean=[104, 117, 123],\n",
    "                                 swapRB=True, crop=False,\n",
    "                                 ddepth=cv2.CV_32F) # nd.array\n",
    "    \n",
    "faceBoxes = {}\n",
    "face_net.setInput(blob=frame_blob)\n",
    "face_detection = face_net.forward()\n",
    "# print(face_detection.shape[2])\n",
    "for i in range(face_detection.shape[2]):\n",
    "    confidence = face_detection[0, 0, i, 2]\n",
    "    if confidence > CONF_THRES:\n",
    "        x1 = face_detection[0, 0, i, 3] * frameWidth\n",
    "        y1 = face_detection[0, 0, i, 4] * frameHeight\n",
    "\n",
    "        x2 = face_detection[0, 0, i, 5] * frameWidth\n",
    "        y2 = face_detection[0, 0, i, 6] * frameHeight\n",
    "\n",
    "        faceBoxes[confidence] = [x1, y1, x2, y2]\n",
    "\n",
    "    if faceBoxes:\n",
    "        gener, age = predict_attribute(face=faceBoxes[max(faceBoxes.keys())], frame=frame)\n",
    "        print(gener, age)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.utils.parser import get_config\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "half = False\n",
    "\n",
    "# Load deepsort\n",
    "cfg = get_config()\n",
    "cfg.merge_from_file('deep_sort/configs/deep_sort.yaml')\n",
    "deepsort = DeepSort('osnet_x0_25',\n",
    "                    max_dist=cfg.DEEPSORT.MAX_DIST,\n",
    "                    max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n",
    "                    max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n",
    "                    use_cuda=False)\n",
    "\n",
    "model = DetectMultiBackend(weights=yolov5x, device='cpu', dnn=False)\n",
    "\n",
    "dataset_img = LoadImages(test_img, img_size=640, stride=32, auto=False)\n",
    "\n",
    "for frame_idx, (path, img, im0s, vid_cap, s) in enumerate(dataset_img):\n",
    "    img = torch.tensor(img, dtype=torch.float32, device='cpu')\n",
    "    ori_img = img.numpy()\n",
    "    _img = img.half() if half else img\n",
    "    _img /= 255.0\n",
    "    if _img.ndimension() == 3:\n",
    "        _img = img.unsqueeze(0)\n",
    "\n",
    "    pred = model(_img, augment=False, visualize=False)\n",
    "    _pred = non_max_suppression(pred, 0.03, 0.5, 0, False, max_det=500)\n",
    "\n",
    "    print(f'_pred: {_pred}')\n",
    "    plt.imshow(ori_img.T)\n",
    "    plt.show()\n",
    "    \n",
    "    for i, det in enumerate(_pred):\n",
    "        path, im0, _ = path, im0s.copy(), getattr(dataset_img, 'frame', 0)\n",
    "        \n",
    "        annotator = Annotator(im0, line_width=2, pil=not ascii)\n",
    "        w, h = im0.shape[1], im0.shape[0]\n",
    "        \n",
    "        # img => (3, 640, 640)\n",
    "        # im0 => (720, 1280, 3) -> (3, 720, 1280)\n",
    "        det[:, :4] = scale_coords(_img.shape[2:], det[:, :4], im0.shape).round()\n",
    "        # print(_img.shape)\n",
    "        \n",
    "        xywhs = xyxy2xywh(det[:, 0:4])\n",
    "        confs = det[:, 4]\n",
    "        _class = det[:, 5]\n",
    "        # _xywhs = xywhs.tolist()[0]\n",
    "        # _xywhs = [ int(n) for n in _xywhs ]\n",
    "        # x1, y1, x2, y2 = _xywhs\n",
    "        # deepsort output\n",
    "        # outputs = deepsort.update(xywhs, confs, _class, ori_img)\n",
    "        # print(outputs)\n",
    "        # cv2.imshow('frame', im0)\n",
    "        # numpy_im0 = im0.numpy()\n",
    "        plt.imshow(im0)\n",
    "        plt.show()\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamflow-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
