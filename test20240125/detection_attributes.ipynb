{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the case of attribute detection, check whether performance improves</b>\n",
    "* case: if the number of simultaneous detections is reduced. \n",
    "* Please indicate the number of simultaneous detections of people and the performance numerically.\n",
    "\n",
    "cases:<br>\n",
    "    1. camera distance - ()<br>\n",
    "    2. camera focus<br>\n",
    "    3. people in frame<br>\n",
    "        i. with face<br>\n",
    "        ii. without face<br>\n",
    "        iii. person count -> (2 - 5), (5 - 10), (10 - 20), (20 - 50)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "sys.path.insert(0, './yolov5')\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.datasets import LoadImages, LoadStreams\n",
    "from yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh, xyxy2xywhn\n",
    "from yolov5.utils.torch_utils import select_device, time_sync\n",
    "from yolov5.utils.plots import Annotator, colors\n",
    "import torch\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"attributes-models/deploy.prototxt\" in function 'ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m genderProto \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes-models\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender_deploy.prototxt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m genderModel \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes-models\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender_net.caffemodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m face_net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNetFromCaffe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaceProto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaceModel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m age_net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNetFromCaffe(ageProto, ageModel)\n\u001b[1;32m     22\u001b[0m gender_net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNetFromCaffe(genderProto, genderModel)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"attributes-models/deploy.prototxt\" in function 'ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DIR_PATH = os.getcwd()\n",
    "TEST_IMAGE_PATH = os.path.join(DIR_PATH, 'test-images')\n",
    "YOLO_MODELS_PATH = os.path.join(DIR_PATH, 'yolo-models')\n",
    "\n",
    "# Load yolo models\n",
    "yolov5x = os.path.join(YOLO_MODELS_PATH, 'yolov5x.pt')\n",
    "yolov5n = os.path.join(YOLO_MODELS_PATH, 'yolov5n.pt')\n",
    "yolo5x6_b20231223 = os.path.join(YOLO_MODELS_PATH, 'yolo5x6_b20231223.pt')\n",
    "yolov5x6_b20240119_ct = os.path.join(YOLO_MODELS_PATH, 'yolov5x6_b20240119_ct.pt')\n",
    "yolov5x6_l20240119_ct = os.path.join(YOLO_MODELS_PATH, 'yolov5x6_l20240119_ct.pt')\n",
    "\n",
    "# Load attributes model\n",
    "faceProto = os.path.join('attributes-models', 'deploy.prototxt')\n",
    "faceModel =  os.path.join('attributes-models', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "ageProto = os.path.join('attributes-models', 'age_deploy.prototxt')\n",
    "ageModel = os.path.join('attributes-models', 'age_net.caffemodel')\n",
    "genderProto = os.path.join('attributes-models', 'gender_deploy.prototxt')\n",
    "genderModel = os.path.join('attributes-models', 'gender_net.caffemodel')\n",
    "\n",
    "face_net = cv2.dnn.readNetFromCaffe(faceProto, faceModel)\n",
    "age_net = cv2.dnn.readNetFromCaffe(ageProto, ageModel)\n",
    "gender_net = cv2.dnn.readNetFromCaffe(genderProto, genderModel)\n",
    "\n",
    "age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "gender_list = ['Male', 'Female']\n",
    "undefined = 'undefined'\n",
    "\n",
    "# Load test images\n",
    "test_img_1 = os.path.join(TEST_IMAGE_PATH, 'test1.png')\n",
    "test_img_2 = os.path.join(TEST_IMAGE_PATH, 'test2.png')\n",
    "test_img_3 = os.path.join(TEST_IMAGE_PATH, 'test3.png')\n",
    "test_img = os.path.join(TEST_IMAGE_PATH, 'test.jpg')\n",
    "\n",
    "\n",
    "IMGSZ = 1280\n",
    "RTSP_URL = 'rtsp://admin:L2140092@192.168.53.48:554/cam/realmonitor?channel=1&subtype=00'\n",
    "source = RTSP_URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(RTSP_URL):\n",
    "    cap = cv2.VideoCapture(RTSP_URL)\n",
    "\n",
    "    if cap.isOpened() is False:\n",
    "        print('[exit] can not access the camera.')\n",
    "        exit(0)\n",
    "\n",
    "    while True:\n",
    "        grabbed, frame = cap.read()\n",
    "\n",
    "        if cv2.waitKey(1) and 0xFF == ord('d'):\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# yolo_model = DetectMultiBackend(yolov5x, device='cpu', dnn=True)\n",
    "# yolo_model = torch.hub.load(model=yolov5x)\n",
    "yolo_model = torch.hub._load_local('./yolov5/', 'custom', path=yolov5x)\n",
    "frame = cv2.imread(test_img_1)\n",
    "pred = yolo_model(frame)\n",
    "\n",
    "# print(pred.pandas().xyxy[0])\n",
    "# _pred = non_max_suppression(pred, 0.03, 0.5, 0, False, 500)\n",
    "\n",
    "pred_xyxy = pred.xyxy\n",
    "pred_xywh = pred.xywh\n",
    "\n",
    "pred_xyxy_norm = pred.xyxyn\n",
    "pred_xywh_norm = pred.xywhn\n",
    "\n",
    "# print(f\"\"\"\n",
    "# pred_xyxy: {pred_xyxy}\\n\n",
    "# pred_xywh: {pred_xywh}\\n\\n\n",
    "\n",
    "# pred_xyxy_norm: {pred_xyxy_norm}\\n\n",
    "# pred_xywh_norm: {pred_xywh_norm}\n",
    "# \"\"\")\n",
    "\n",
    "xywh_numpy = pred_xywh[0].tolist()\n",
    "\n",
    "xywhs = xywh_numpy[0][0:4]\n",
    "confs = xywh_numpy[0][4]\n",
    "_class = xywh_numpy[0][5]\n",
    "x1, y1, x2, y2 = xywhs\n",
    "\n",
    "# print(f'({x1}, {y1}): ({x2}, {y2})')\n",
    "# frame\n",
    "# print(pred.pandas().xyxy)\n",
    "# new_frame = np.reshape(frame, -1)\n",
    "# h, w = frame.shape[:2]\n",
    "\n",
    "# length\n",
    "# print(f'pred.pred: \\n{len(pred.pred[0])}')\n",
    "pred_xyxy = pred_xyxy[0].numpy()\n",
    "# print(len(pred_xyxy))\n",
    "person_detect_count = 0\n",
    "i = 0\n",
    "for pred_obj in pred_xyxy:\n",
    "    for property in pred_obj:\n",
    "        if property == 0.0:\n",
    "            person_detect_count += 1\n",
    "\n",
    "\n",
    "print(person_detect_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.utils.parser import get_config\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pybboxes as pbx\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "half = False\n",
    "\n",
    "height = frame.shape[0]\n",
    "width = frame.shape[1]\n",
    "img = np.zeros((height, width), 'uint8')\n",
    "\n",
    "# Load deepsort\n",
    "cfg = get_config()\n",
    "cfg.merge_from_file('deep_sort/configs/deep_sort.yaml')\n",
    "deepsort = DeepSort('osnet_x0_25',\n",
    "                    max_dist=cfg.DEEPSORT.MAX_DIST,\n",
    "                    max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n",
    "                    max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n",
    "                    use_cuda=False)\n",
    "\n",
    "model = DetectMultiBackend(weights=yolov5x, device='cpu', dnn=False)\n",
    "\n",
    "dataset_img = LoadImages(test_img_1, img_size=640, stride=32, auto=False)\n",
    "\n",
    "for frame_idx, (path, img, im0s, vid_cap, s) in enumerate(dataset_img):\n",
    "    img = torch.tensor(img, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "    ori_img = img.numpy()\n",
    "    _img_test = img.squeeze().permute(1, 2, 0)\n",
    "    \n",
    "    _img = img.half() if half else img\n",
    "    _img /= 255.0\n",
    "    if _img.ndimension() == 3:\n",
    "        _img = img.unsqueeze(0)\n",
    "\n",
    "    pred = model(_img, augment=False, visualize=False)\n",
    "    _pred = non_max_suppression(pred, 0.03, 0.5, 0, False, max_det=500)\n",
    "\n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "    # _pred[0].save()\n",
    "    print(f'Lenght: {len(_pred[0])}')\n",
    "    \n",
    "    for i, det in enumerate(_pred):\n",
    "        path, im0, _ = path, im0s.copy(), getattr(dataset_img, 'frame', 0)\n",
    "        \n",
    "        annotator = Annotator(im0, line_width=2, pil=not ascii)\n",
    "        w, h = im0.shape[1], im0.shape[0]\n",
    "        \n",
    "        # img => (3, 640, 640)\n",
    "        # im0 => (720, 1280, 3) -> (3, 720, 1280)\n",
    "        det[:, :4] = scale_coords(_img.shape[2:], det[:, :4], im0.shape).round()\n",
    "        # print(_img.shape)\n",
    "        \n",
    "        xywhs = xyxy2xywh(det[:, 0:4])\n",
    "        xyxy = det[:, 0:4].numpy()\n",
    "        confs = det[:, 4]\n",
    "        _class = det[:, 5]\n",
    "\n",
    "        # xywh_norm = tuple(xyxy2xywhn(det[:, 0:4]).numpy()[0])\n",
    "        # print(f'_img: {_img.shape}')\n",
    "\n",
    "        # cv2.rectangle(frame, (stX, stY), (enX, enY), (0, 255, 0), 2)\n",
    "        # plt.imshow(im0)\n",
    "        # plt.show()\n",
    "        bbox = xywhs.numpy()[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h, w) = frame.shape[: 2]\n",
    "\n",
    "MEAN_VALUE_1 = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "MEAN_VALUE_2 = (104.0,177.0,123.0)\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), MEAN_VALUE_2)\n",
    "\n",
    "face_net.setInput(blob=blob)\n",
    "de = face_net.forward()\n",
    "# print(f'detection: {de}\\n shape: {de.shape}')\n",
    "# print(f'h: {h}, w: {w}\\n frame-shape: {frame.shape}')\n",
    "# de.shape # (1, 1, 200, 7)\n",
    "face_boxes = {}\n",
    "for i in range(0, de.shape[2]):\n",
    "    confidance = de[0, 0, i, 2]\n",
    "    if confidance > 0.5:\n",
    "        box = de[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "        face_boxes[confidance] = [startX, startY, endX, endY]\n",
    "        # face_boxes.update({ confidance: list(startX, startY, endX, endY) })\n",
    "        # print(startX, startY, endX, endY)\n",
    "        txt = str(confidance * 100)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, txt, (startX, y), cv2.FONT_ITALIC, 2.5, (0, 255, 0), 2)\n",
    "        print(f'Box: {box}')\n",
    "\n",
    "OiH = frame.shape[0]\n",
    "OiW = frame.shape[1]\n",
    "\n",
    "image = cv2.resize(frame, (round(OiW * 0.3), round(OiH * 0.3)))        \n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(de[0, 0, 21, 3:7] * np.array([w, h, w, h]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRES = 0.05\n",
    "\n",
    "def predict_attribute(face, frame):\n",
    "    gender = undefined\n",
    "    age = undefined\n",
    "    padding = 20\n",
    "    \n",
    "    # print(face[0])\n",
    "    # print(face[1])\n",
    "    # print(face[2])\n",
    "    # print(face[3])\n",
    "    face_img = frame[max(0, face[1] - padding): min(face[3] + padding, frame.shape[0] - 1),\n",
    "                     max(0, face[0] - padding): min(face[2] + padding, frame.shape[1] - 1)]\n",
    "    \n",
    "    print(face_img)\n",
    "\n",
    "    # print(frame[max(0, face[1] - padding): min(face[3] + padding, frame.shape[0] - 1)])\n",
    "    # print()\n",
    "    # print(max(0, face[0] - padding))\n",
    "    # print(min(face[2] + padding, frame.shape[1] - 1))\n",
    "    if len(face_img) != 0:\n",
    "        blob = cv2.dnn.blobFromImage(face_img, scalefactor=1.0, size=(227, 227), mean=MODEL_MEAN_VALUES, swapRB=False)\n",
    "        gender_net.setInput(blob)\n",
    "        gender_preds = gender_net.forward()\n",
    "        gender = gender_list[gender_preds[0].argmax()]\n",
    "\n",
    "        age_net.setInput(blob)\n",
    "        age_preds = age_net.forward()\n",
    "        age = age_list[age_preds[0].argmax()]\n",
    "        \n",
    "        return gender, age\n",
    "    # print(face_img)\n",
    "    # return 'hello', 'world'\n",
    "\n",
    "# get_face(frame)\n",
    "# frameOpenCvDnn = pred_xywh_norm # (720, 1280, 3)\n",
    "frameOpenCvDnn = im0\n",
    "frameHeight = im0.shape[0]\n",
    "frameWidth = im0.shape[1]\n",
    "\n",
    "\n",
    "frame_blob = cv2.dnn.blobFromImage(frameOpenCvDnn, scalefactor=1.0,\n",
    "                                 size=(300, 300), mean=[104, 117, 123],\n",
    "                                 swapRB=True, crop=False,\n",
    "                                 ddepth=cv2.CV_32F) # nd.array\n",
    "    \n",
    "faceBoxes = {}\n",
    "face_net.setInput(blob=frame_blob)\n",
    "face_detection = face_net.forward()\n",
    "# print(face_detection.shape[2])\n",
    "for i in range(face_detection.shape[2]):\n",
    "    confidence = face_detection[0, 0, i, 2]\n",
    "    if confidence > CONF_THRES:\n",
    "        x1 = face_detection[0, 0, i, 3] * frameWidth\n",
    "        y1 = face_detection[0, 0, i, 4] * frameHeight\n",
    "\n",
    "        x2 = face_detection[0, 0, i, 5] * frameWidth\n",
    "        y2 = face_detection[0, 0, i, 6] * frameHeight\n",
    "\n",
    "        faceBoxes[confidence] = [x1, y1, x2, y2]\n",
    "\n",
    "    if faceBoxes:\n",
    "        gener, age = predict_attribute(face=faceBoxes[max(faceBoxes.keys())], frame=im0)\n",
    "        print(gener, age)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamflow-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
