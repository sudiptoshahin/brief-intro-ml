{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_ROOT = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "constant = {\n",
    "    'faceModel': os.path.join(MODEL_ROOT, 'res10_300x300_ssd_iter_140000.caffemodel'),\n",
    "    'faceProto': os.path.join(MODEL_ROOT, 'deploy.prototxt'),\n",
    "    'ageModel': os.path.join(MODEL_ROOT, 'age_net.caffemodel'),\n",
    "    'ageProto': os.path.join(MODEL_ROOT, 'age_deploy.prototxt'),\n",
    "    'genderModel': os.path.join(MODEL_ROOT, 'gender_net.caffemodel'),\n",
    "    'genderProto': os.path.join(MODEL_ROOT, 'gender_deploy.prototxt')\n",
    "}\n",
    "\n",
    "age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "gender_list = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Load attribute models\n",
    "face_proto = constant['faceProto']\n",
    "face_model = constant['faceModel']\n",
    "\n",
    "age_proto = constant['ageProto']\n",
    "age_model = constant['ageModel']\n",
    "\n",
    "gender_proto = constant['genderProto']\n",
    "gender_model = constant['genderModel']\n",
    "\n",
    "face_net = cv2.dnn.readNetFromCaffe(face_proto, face_model)\n",
    "age_net = cv2.dnn.readNetFromCaffe(age_proto, age_model)\n",
    "gender_net = cv2.dnn.readNetFromCaffe(gender_proto, gender_model)\n",
    "\n",
    "MODEL_MEAN_VALUES = MODEL_MEAN_VALUES\n",
    "gender_list = gender_list\n",
    "age_list = age_list\n",
    "\n",
    "padding = 20\n",
    "\n",
    "\n",
    "def get_face(frame, conf_threshold=0.75):\n",
    "    frame_opencv_dnn = frame.copy()\n",
    "    frame_height, frame_width, _ = frame_opencv_dnn.shape\n",
    "\n",
    "    # optimized_img = cv2.medianBlur(frame_opencv_dnn, 5)\n",
    "    #\n",
    "    # cv2.imshow('op', frame_opencv_dnn)\n",
    "\n",
    "    gender = None\n",
    "    age = None\n",
    "\n",
    "    try:\n",
    "        blob = cv2.dnn.blobFromImage(frame_opencv_dnn, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "\n",
    "        # face_boxes = {}\n",
    "\n",
    "        face_net.setInput(blob)\n",
    "        detections = face_net.forward()\n",
    "\n",
    "        # gender_net.setInput(blob)\n",
    "        # gender_prediction = gender_net.forward()\n",
    "        # gender = gender_list[gender_prediction[0].argmax()]\n",
    "        #\n",
    "        # age_net.setInput(blob)\n",
    "        # age_prediction = age_net.forward()\n",
    "        # age = age_list[age_prediction[0].argmax()]\n",
    "\n",
    "        confidences = detections[0, 0, :, 2]\n",
    "        indices = np.where(confidences >= conf_threshold)[0]\n",
    "\n",
    "        # indices = np.where(confidences > conf_threshold)[0]\n",
    "        #\n",
    "        boxes = detections[0, 0, indices, 3:7] * np.array([frame_width, frame_height, frame_width, frame_height])\n",
    "        boxes = boxes.astype(int)\n",
    "        #\n",
    "        confidences_filtered = confidences[indices]\n",
    "        #\n",
    "        face_boxes = {confidence: box.tolist() for confidence, box in zip(confidences_filtered, boxes)}\n",
    "\n",
    "\n",
    "        # for i in indices:\n",
    "        #     box = detections[0, 0, i, 3:7] * np.array([frame_width, frame_height, frame_width, frame_height])\n",
    "        #     x1, y1, x2, y2 = box.astype(int)\n",
    "        #     confidence = confidences[i]\n",
    "        #     face_boxes[confidence] = [x1, y1, x2, y2]\n",
    "\n",
    "            # x1 = int(detections[0, 0, i, 3] * frame_width)\n",
    "            # y1 = int(detections[0, 0, i, 4] * frame_height)\n",
    "            # x2 = int(detections[0, 0, i, 5] * frame_width)\n",
    "            # y2 = int(detections[0, 0, i, 6] * frame_height)\n",
    "            #\n",
    "            # confidence = confidences[i]\n",
    "            # face_boxes[confidence] = [x1, y1, x2, y2]\n",
    "\n",
    "        if face_boxes:\n",
    "            max_confidence = max(face_boxes.keys())\n",
    "            gender, age = predict_attribute(face=face_boxes[max_confidence], frame=frame)\n",
    "        return [gender, age]\n",
    "\n",
    "        # for i in range(detections.shape[2]):\n",
    "        #     confidence = detections[0, 0, i, 2]\n",
    "        #     if confidence > conf_threshold:\n",
    "        #         x1 = int(detections[0, 0, i, 3] * frame_width)\n",
    "        #         y1 = int(detections[0, 0, i, 4] * frame_height)\n",
    "        #         x2 = int(detections[0, 0, i, 5] * frame_width)\n",
    "        #         y2 = int(detections[0, 0, i, 6] * frame_height)\n",
    "        #\n",
    "        #         face_boxes[confidence] = [x1, y1, x2, y2]\n",
    "        #\n",
    "        # if face_boxes:\n",
    "        #     gender, age = predict_attribute(face=face_boxes[max(face_boxes.keys())], frame=frame)\n",
    "        #     return [gender, age]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def predict_attribute(face, frame):\n",
    "    \"\"\"\n",
    "    Predict gender and age based on the detected face region in the frame.\n",
    "\n",
    "    Args:\n",
    "    face (list): List containing coordinates [x1, y1, x2, y2] of the detected face.\n",
    "    frame (numpy.ndarray): Frame containing the detected face region.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing predicted gender and age based on the detected face.\n",
    "    (gender, age) will be returned. If face_img is empty or an error occurs during prediction,\n",
    "    'undefined' will be returned for both gender and age.\n",
    "\n",
    "    This function predicts the gender and age of a detected face using pre-trained models.\n",
    "    It extracts the face region from the frame and performs inference using gender_net and age_net.\n",
    "    \"\"\"\n",
    "    face_img = frame[max(0, face[1] - padding): min(face[3] + padding, frame.shape[0] - 1), max(0, face[0] - padding):min(face[2] + padding, frame.shape[1] - 1)]\n",
    "\n",
    "    gender = None\n",
    "    age = None\n",
    "\n",
    "    if len(face_img) != 0:\n",
    "        blob2 = cv2.dnn.blobFromImage(face_img, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "\n",
    "        gender_net.setInput(blob2)\n",
    "        gender_prediction = gender_net.forward()\n",
    "        gender = gender_list[gender_prediction[0].argmax()]\n",
    "\n",
    "        age_net.setInput(blob2)\n",
    "        age_prediction = age_net.forward()\n",
    "        age = age_list[age_prediction[0].argmax()]\n",
    "\n",
    "    return gender, age\n",
    "\n",
    "\n",
    "def make_shadow(im0, coords):\n",
    "    \"\"\"\n",
    "    Create a shadow effect on an image based on the provided coordinates.\n",
    "\n",
    "    Args:\n",
    "    im0 (numpy.ndarray): The input image.\n",
    "    coords (list): List of coordinates defining the shape for which the shadow will be created.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An image with the applied shadow effect.\n",
    "    \"\"\"\n",
    "    # Step 1: Make a copy of that image\n",
    "    shadow = im0.copy()\n",
    "\n",
    "    # Step 2: Create a mask for the filled shape\n",
    "    mask = np.zeros_like(im0)  # Create a black mask with the same size as the image\n",
    "    cv2.fillPoly(mask, [coords], (255, 255, 255))  # Fill the shape with white in the mask\n",
    "\n",
    "    # Step 3: Create a transparent shadow image\n",
    "    shadow_alpha = 0.5  # Adjust transparency here (0.0 to 1.0)\n",
    "    shadow[:, :, 3] = shadow_alpha * 255\n",
    "\n",
    "    # Step 4: Overlay the shadow on the original image using the mask\n",
    "    result = cv2.add(im0, shadow, mask=mask)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@493.295] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video2): can't open camera by index\n",
      "[ERROR:0@493.413] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     ret,frame\u001b[38;5;241m=\u001b[39mvideo\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 6\u001b[0m     frame,bboxs\u001b[38;5;241m=\u001b[39m\u001b[43mget_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxs:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# face=frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         face \u001b[38;5;241m=\u001b[39m frame[\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m,bbox[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mpadding):\u001b[38;5;28mmin\u001b[39m(bbox[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m+\u001b[39mpadding,frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m,bbox[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mpadding):\u001b[38;5;28mmin\u001b[39m(bbox[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39mpadding, frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mget_face\u001b[0;34m(frame, conf_threshold)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_face\u001b[39m(frame, conf_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Detect faces and attributes in the provided frame.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    It predicts the gender and age of the detected face using another function 'predict_attribute'.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     frame_opencv_dnn \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m     42\u001b[0m     frame_height, frame_width, _ \u001b[38;5;241m=\u001b[39m frame_opencv_dnn\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# optimized_img = cv2.medianBlur(frame_opencv_dnn, 5)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# cv2.imshow('op', frame_opencv_dnn)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "video=cv2.VideoCapture(2)\n",
    "padding=20\n",
    "\n",
    "while True:\n",
    "    ret,frame=video.read()\n",
    "    frame,bboxs=get_face(frame)\n",
    "    for bbox in bboxs:\n",
    "        # face=frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "        blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        gender_model.setInput(blob)\n",
    "        genderPred=gender_model.forward()\n",
    "        gender=gender_list[genderPred[0].argmax()]\n",
    "\n",
    "\n",
    "        age_model.setInput(blob)\n",
    "        agePred=age_model.forward()\n",
    "        age=age_list[agePred[0].argmax()]\n",
    "\n",
    "\n",
    "        label=\"{},{}\".format(gender,age)\n",
    "        cv2.rectangle(frame,(bbox[0], bbox[1]-30), (bbox[2], bbox[1]), (0,255,0),-1) \n",
    "        cv2.putText(frame, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2,cv2.LINE_AA)\n",
    "    cv2.imshow(\"Age-Gender\",frame)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==ord('q'):\n",
    "        break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamflow-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
